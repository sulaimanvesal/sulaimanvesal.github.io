<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>ASAPNet</title>
	<meta property="og:image" content=""/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Unsupervised Adaptation of Point-Clouds and Entropy Minimisation for Multi-modal Cardiac-MR Segmentation" />
	<meta property="og:description" content="Sulaiman Vesal et al.,  ArXiv 2021." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head> -->

<body>
	<br>
	<center>
		<span style="font-size:36px">Unsupervised Adaptation of Point-Clouds and Entropy Minimisation for Multi-modal Cardiac-MR Segmentation</span>
		<table align=center width=1100px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=110px>
						<center>
							<span style="font-size:24px"><a href="https://stamarot.webgr.technion.ac.il/">Sulaiman Vesal</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://www.mgharbi.com">Mingxuan Gu</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://rkosti.github.io/">Ronak Kosti</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://lme.tf.fau.de/person/maier/"> Andreas Maier </a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://eps.leeds.ac.uk/computing/staff/1846/dr-nishant-ravikumar/">Nishant Ravikumar</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=500px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href=''>[GitHub](soon)</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:400px" src="./resources/runtime_vs_imgsize_Mpix_v15.png"/>
					</center>
				</td>
			</tr>
		</table>
		<br>
		<table align=center width=850px>
			<tr align=justify>
				<td>
					Our novel model, designed with multi-task point-cloud and segmentation network enables bridging the gap between two differet imaging domains 
					at significantly lower runtimes than existing methods, while maintaining high segmentation accuracy.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr align=justify>
			<td>
		Deep learning models are sensitive to domain shift phenomena. A model trained on images from one domain cannot generalise well when tested on 
				images from a different domain, despite capturing similar anatomical structures. It is mainly because the data distribution between 
				the two domains is different. Moreover, creating annotation for every new modality is a tedious and time-consuming task, which 
				also suffers from high inter- and intra- observer variability. Unsupervised domain adaptation (UDA) methods intend to reduce the 
				gap between source and target domains by leveraging source domain labelled data to generate labels for the target domain. However, 
				current state-of-the-art (SOTA) UDA methods demonstrate degraded performance when there is insufficient data in source and target domains. 
				In this paper, we present a novel UDA method for multi-modal cardiac image segmentation. The proposed method is based on adversarial 
				learning and adapts network features between source and target domain in different spaces. The paper introduces an end-to-end framework 
				that integrates: a) entropy minimisation, b) output feature space alignment and c) a novel point-cloud shape adaptation based on latent 
				features learned by the segmentation model. We validated our method on two cardiac datasets by adapting from the annotated source domain, 
				bSSFP-MRI (balanced Steady-State Free Procession-MRI), to the unannotated target domain, LGE-MRI (Late-gadolinium enhance-MRI), for 
				the multi-sequence dataset; and from MRI (source) to CT (target) for the cross-modality dataset. The results highlighted that by enforcing adversarial 
				learning in different parts of the network, the proposed method delivered promising performance, compared to other SOTA methods.
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=850px>
		<tr>
			<td width=850px>
				<center>
					<img class="round" style="width:700px" src="./resources/ctmr_output.png"/>
				</center>
			</td>
		</tr>
	</table>
	<hr>
		

	<center><h1>Implementation</h1></center>
	<table align=center width=800px>
		<tr><center>
			<span style="font-size:24p	x">&nbsp;<a href=''>[GitHub](soon)</a>
			</center>
		</span>
	</table>
	<br>

	<table align=center width=850px>
		<tr>
			<td width=850px>
				<center>
					<img class="round" style="width:700px" src="./resources/git_framework.png"/>
				</center>
			</td>
		</tr>
	</table>
	
	<table align=center width=850px>
		<center>
			<tr align=justify>
				<td>
					A single image from the source domain or the target domain fed 
					into its corresponding domain-specific DR-UNet segmentation network, 
					which has shared weights. The DR-UNet encoder extract high-level features for both the source 
					and target domains. Then the features are sent to the decoder for segmentation and point-net to 
					generate point-cloud. The point-cloud is fed to <b>D_3</b> for shape alignment. 
					The output probability of DR-UNet for the source domain is trained in a supervised manner. 
					Then, we send the \textit{softmax} output simultaneously to output-space alignment 
					and entropy minimisation discriminators. The domain classifier networks, 
					 <b>D_1</b> and  <b>D_2</b>, then differentiates whether its input is from source domain or target domain.
			</tr>
				</td>
		</center>
	</table>

	<hr>
	<table align=center width=550px>
		<center><h1>Paper</h1></center>
		<tr align=justify>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/ASAPNet-01.png"/></a></td>
			<td><span style="font-size:14pt">T. Rott Shaham, M. Gharbi, R. Zhang, E. Shechtman, T. Michaeli<br>
				<b>Spatially-Adaptive Pixelwise Networks for Fast Image Translation</b><br>
				<!--ArXiv, 2020<br> -->
				(<a href="">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>
	
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>
	
	<hr>	
	<table align=center width=900px>
		<center><h1>References</h1></center>
		<tr align=justify>
			<td width=400px>
				<left>
				Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang and Hongsheng Li, 
				<b>Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis,</b>	
				NeurIPS 2019
<br><br>				
				 Taesung Park, Ming-Yu Liu, Ting-Chun Wang and Jun-Yan Zhu,    
				<b>Semantic Image Synthesis with Spatially-Adaptive Normalization,</b>	
				CVPR 2019	
<br><br>				
				Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro,				
				<b> High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,</b>	
				CVPR 2018	
				</left>
<br><br>				
				 Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun,    
				<b>Semi-parametric Image Synthesis,</b>	
				CVPR 2018	
				</left>
<br><br>				
				 Qifeng Chen and Vladlen Koltun,    
				<b>Photographic Image Synthesis with Cascaded Refinement Networks,</b>	
				ICCV 2017	
				</left>
			</td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

